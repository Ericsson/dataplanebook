<!DOCTYPE html>

<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Work Scheduling &#8212; Data Plane Software Design 0.0.2 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Ethernet Devices" href="../eth.html" />
    <link rel="prev" title="Threading" href="../threading/threading.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="work-scheduling">
<span id="id1"></span><h1>Work Scheduling<a class="headerlink" href="#work-scheduling" title="Permalink to this heading">¶</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>This chapter investigates how to organize processing in the data
plane.</p>
<p>The primary concern is the distribution of work across CPU cores and
accelerators in an as cycle- and energy-efficiency and scalable was as
possible, while maintaining correctness and the appropriate
<a class="reference internal" href="../glossary.html#term-Network-function"><span class="xref std std-term">network function</span></a>-level black-box behavior.</p>
<p>As discussed in the <a class="reference internal" href="../threading/threading.html#threading"><span class="std std-ref">chapter on threading</span></a>, a data
plane application is generally required to set up a system of
<a class="reference internal" href="../glossary.html#term-Operating-system-thread"><span class="xref std std-term">threads</span></a>-pinned-to-<a class="reference internal" href="../glossary.html#term-CPU-core"><span class="xref std std-term">cores</span></a> — a scheme which effectively disables operating system-level
<a class="reference internal" href="../glossary.html#term-Multitasking"><span class="xref std std-term">multitasking</span></a>.</p>
<p>With the standard vehicle for load balancing thrown out the window,
the data plane application needs a replacement. That replacement is
the topic of this chapter.</p>
<p>The related but distinct question of how to decouple different
protocol layers (and other modules) in the data plane application will
be covered in a future chapter of <a class="reference internal" href="../modularization.html#modularization"><span class="std std-ref">Modularization</span></a>. The DPDK
<a class="reference internal" href="../glossary.html#term-Service-cores-framework"><span class="xref std std-term">service cores framework</span></a> will also be the subject of that
future chapter, since its primary function is decoupling (and
<a class="reference internal" href="../glossary.html#term-Concurrency"><span class="xref std std-term">concurrency</span></a>).</p>
</section>
<section id="basic-concepts">
<h2>Basic Concepts<a class="headerlink" href="#basic-concepts" title="Permalink to this heading">¶</a></h2>
<p>This section introduces essential terminology, enabling a concise and
precise discussion on data plane work scheduling.</p>
<section id="item-of-work">
<h3>Item of Work<a class="headerlink" href="#item-of-work" title="Permalink to this heading">¶</a></h3>
<p>This book uses the term <a class="reference internal" href="../glossary.html#term-Item-of-work"><span class="xref std std-term">item of work</span></a> to denote the smallest
job assigned to an <a class="reference internal" href="../glossary.html#term-EAL-thread"><span class="xref std std-term">EAL thread</span></a>.</p>
<p>An item of work generally consists of two things.</p>
<p>One is the data required to allow the work scheduler to give the item
of work the appropraite treatment. This information will be provided
by the work <em>producer</em>, at the time of work creation.</p>
<section id="producer-information">
<h4>Producer Information<a class="headerlink" href="#producer-information" title="Permalink to this heading">¶</a></h4>
<ol class="arabic simple">
<li><p>Information to the work scheduler can treat this item of work
appropriately.</p></li>
<li><p>Enough information so that the work consumer can perform the work.
1. Data to facilitating the item of work to be dispatched to a function.
1. Pointer to a packet buffer (or a list of packet buffers <a class="footnote-reference brackets" href="#vector" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>) or similar.</p></li>
</ol>
<p>An item of work consists of information on how the work scheduler
should (or must) treat this item, and a pointer to a packet (or a list
of packets <a class="footnote-reference brackets" href="#vector" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>), and some additional meta data, like the
application-level destination (identifying the processing stage).</p>
<p>At a bare minimum, the an item of work handed from the work scheduler
to the receiving party in the application must carry enough
information so that the receiver knows what to do with it.</p>
<p>Similarly, when an item of work is being added to the work scheduler,
it must carry enough information so the that work scheduler knows what
to do with it.</p>
<p>An item of work may also represent a timeout event, a completion
notification from an accelerator, or a request from <a class="reference internal" href="../glossary.html#term-Data-plane-control"><span class="xref std std-term">data plane
control</span></a> to update a table, or retrieve some information about the
state of the data plane.</p>
<p>In DPDK <a class="reference external" href="https://doc.dpdk.org/api/rte__eventdev_8h.html">&lt;rte_eventdev.h&gt;</a> an item of work is
referred to as an <em>event</em>. The same term is used by the <a class="reference external" href="https://openeventmachine.github.io/em-odp/">Open Event
Machine</a>, an
eventdev-like mechanism for <a class="reference internal" href="../intro.html#odp"><span class="std std-ref">Open Data Plane</span></a>.</p>
<p>For non-scheduled data planes, the item of work simply consist of a
pointer to the packet buffer (and its meta data).</p>
<p>In the context of an operating system process scheduler, the item of
work is a runnable process, thread, or (in the case of Linux)
task. For the process scheduler, an item of work always contains a
stack, register state (including a program counter). An item of work
need not, and for the purpose of this chapter, does not.</p>
<p>The way the application communicate details about how an item of work
should be treated is also different between a process scheduler, and
a data plane work scheduler.</p>
</section>
</section>
<section id="work-scheduler">
<h3>Work Scheduler<a class="headerlink" href="#work-scheduler" title="Permalink to this heading">¶</a></h3>
<p>The <em>work scheduler</em> of this book is a data plane fast path function
that distributes <a class="reference internal" href="../glossary.html#term-Item-of-work"><span class="xref std std-term">items of work</span></a> across
<a class="reference internal" href="../glossary.html#term-Fast-path-lcore"><span class="xref std std-term">fast path lcores</span></a>.</p>
<p>The work scheduler takes as input a set of items of work, and decides
how best to distribute the work across available lcores, maintaing the
constraints (e.g., ordering requirements) given by the application.</p>
<p>From a characteristics point of view, the overall goal of the work
scheduler can be summarized as:</p>
<ul class="simple">
<li><p>Maximize throughput (best-case, worst-case, and/or something in-between).</p></li>
<li><p>Minimize <a class="reference internal" href="../glossary.html#term-Wall-clock-latency"><span class="xref std std-term">latency</span></a> (average and/or at the
tail end).</p></li>
<li><p>Maximize resource efficiency (e.g., power, CPU cycles, or DRAM bandwidth).</p></li>
<li><p>Maintain fairness (or more general, maintain appropriate quality
of service, which may not be fair at all).</p></li>
<li><p>Provide the appropriate application programming model, to maximize
developer efficiency and minimize development cost.</p></li>
</ul>
<p>Depending on the application, different weights will be placed on the
different work scheduler sub goals.</p>
<p>Conceptually, the data plane work scheduler performs a task very
similar to that of an operating system kernel process scheduler. See
the <a class="reference internal" href="../threading/threading.html#process-scheduler"><span class="std std-ref">Process Scheduler</span></a> section.</p>
<p>What kind of functionality (e.g., treatment) the work scheduler will
provide, and how work is fed into and retrieved from the machinery
varies depending on the work scheduler implementation.</p>
<p>The work scheduler may be implemented in software (in the
<a class="reference internal" href="../glossary.html#term-Data-plane-platform"><span class="xref std std-term">platform</span></a>, or the application), or in
hardware, or a combination of the two.</p>
<p>A data plane work scheduler may take many different forms, ranging
from something that adhere to from <a class="reference external" href="https://doc.dpdk.org/guides/prog_guide/eventdev.html">Eventdev</a> to a
relatively simple function such as <a class="reference internal" href="../glossary.html#term-RSS"><span class="xref std std-term">RSS</span></a> of <a class="reference internal" href="../glossary.html#term-NIC"><span class="xref std std-term">NIC</span></a>
(combined with the appropriate configuration and RX queue setup).</p>
</section>
<section id="work-size">
<h3>Work Size<a class="headerlink" href="#work-size" title="Permalink to this heading">¶</a></h3>
<p>The per-packet <a class="reference internal" href="../glossary.html#term-Processing-latency"><span class="xref std std-term">processing latency</span></a> in the data plane is short
compared to the cost of serving a request in most other types of
network applications. See the section on <a class="reference internal" href="../intro.html#data-plane-applications"><span class="std std-ref">Data Plane Applications</span></a> for more on this topic.</p>
<p>The items of work managed by the data plane work scheduler are
dominated by items directly related to the (partial, or complete)
processing of a particular packet. To improve work scheduling
efficiency (among other things), an item of work may reference
multiple packets. Furthermore, when the work scheduler hands over work
to the application, it may do so in batches, to reduce scheduling
overhead (and give ample opportunity for <a class="reference internal" href="../glossary.html#term-Software-prefetching"><span class="xref std std-term">software
prefetching</span></a>).</p>
<p>However, there also forces that work in the opposite direction, toward
a preference of short (batches of) jobs:</p>
<ul class="simple">
<li><p>There are no means for the work scheduler to interrupt an on-going
execution. This in turn will cause head-of-line blocking, where a
low-priority job may prevent the work scheduler from immediately
scheduling an arriving high-priority job for execution.</p></li>
<li><p>A system could wait to gather a large batch of packets, to reap the
efficiency benefits of batching. <a class="footnote-reference brackets" href="#vpp" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> However, such buffering
will add to the port-to-port <a class="reference internal" href="../glossary.html#term-Wall-clock-latency"><span class="xref std std-term">wall-clock latency</span></a>.</p></li>
</ul>
<p>One often-viable approach is to have a system where limited or no
batching of packet (or items of work) occurs at low load, and
increases with increased system capacity utilization. If the system is
working at the limits of its capacity, a large amount of batching is
usually the best option. At this operating point, the typically
somewhat bursty nature of arriving packets will leave the system, at
least at times, with a backlog, forcing the use of buffering (or
dropping packets). Batching in this scenario allows the system to
exploit the efficiency gains, potentially avoiding further buffering
(and the associated increase in packet delays) to occur.</p>
<p>For systems which can scale hardware resources (e.g., CPU cores) very
quickly, it may be beneficial to keep the system running on
fewer-than-available cores, and keep those cores relatively busy,
leading to batching effects, and in turn higher power efficiency (at
the cost of increased port-to-port wall-time latency experienced by
forwarded packets).</p>
</section>
<section id="flow">
<span id="id5"></span><h3>Flow<a class="headerlink" href="#flow" title="Permalink to this heading">¶</a></h3>
<p>The data plane work scheduler must often track relationships between
an <a class="reference internal" href="../glossary.html#term-Item-of-work"><span class="xref std std-term">item of work</span></a> and other items (see the section on <a class="reference internal" href="#work-ordering"><span class="std std-ref">Work Ordering</span></a> and <a class="reference internal" href="#scheduling-types"><span class="std std-ref">Scheduling Types</span></a>).</p>
<p>For tracking arrival order, or processing order, the concept of a
<em>flow</em> is often useful. The flow of this chapter is often, but does
not have to be, related to some flow-like concept in a network
protocol layer.</p>
<p>For data plane applications implementing some part of the Internet
<a class="reference internal" href="../glossary.html#term-Network-protocol-suite"><span class="xref std std-term">network protocol suite</span></a> flows are often defined in terms IP
packets assoicated with the same TCP connection (or UDP flow), packets
coming in on the same link-layer interface (receiving the same QoS
classification), or all packets going in a particular direction,
between two IP hosts.</p>
<p>It’s often useful to map several network-level flows to a single item
of work flow. This may be done both to reduce the number of flows the
work scheduler must handle, or extend the ordering guarantees given by
the different scheduling types to include more than one flow. One
example is to classify both uplink and downlink TCP frames into the
same atomic flow, to reduce synchronization overhead and cache
locality for data pertaining to both directions, for that TCP
connection.</p>
<p>It’s also often useful to map a single network-level stream of
packets, in one particular direction, to multiple item of work flows,
in an application implementing the pipeline pattern. Each such flow
represent a processing stage. See the <a class="reference internal" href="#pipeline"><span class="std std-ref">Pipeline</span></a> section for more
information.</p>
<p>Note also that such a network-level stream not stay intact across the
pipeline. For example, it may “fan out” and become multiple
independent flows in higher-numbered processing stages.</p>
<p>The term <em>ordering context</em> is sometimes (e.g., in <a class="reference internal" href="../glossary.html#term-NPU"><span class="xref std std-term">NPUs</span></a>)
used for an item of work flow.</p>
</section>
<section id="work-ordering">
<span id="id6"></span><h3>Work Ordering<a class="headerlink" href="#work-ordering" title="Permalink to this heading">¶</a></h3>
<p>A <a class="reference internal" href="../glossary.html#term-Network-function"><span class="xref std std-term">network function</span></a> performing packet forwarding (in a general
sense) is almost always required to maintain some kind of order of
packets coming in, and the corresponding packets going out. In other
words, when packets arrive in some well-defined order at the
<a class="reference internal" href="../glossary.html#term-Network-function"><span class="xref std std-term">network function</span></a> ingress, any packets produced as a results of
processing those packets, should egress the system in the same order.</p>
<p><a class="reference internal" href="../glossary.html#term-Network-function"><span class="xref std std-term">Network function</span></a>-level packet ordering requirements directly
translate into a set of requirements on the data plane work scheduler
(and associated machinery, like a packet-to-flow classification
function).</p>
<p>Generally, causing reordering is not strictly prohibited, and its
better to deliver out-of-order packets than to drop them. For example,
in an IP network, a stream of packets originating from some particular
host may be reordered as they traverse the network. Reordering of
packets may result in serious performance degradations.</p>
<p>For example, <a class="reference internal" href="../glossary.html#term-Layer-4"><span class="xref std std-term">Transport layer</span></a> protocols may infer
packet loss from the arrival of out-of-order packets, and signaling to
the remote peer retransmit the data.</p>
<p>At a minimum, reordering incurs an addition processing cost on the end
systems.</p>
<p>Typically, the data plane application need not maintain a total order
between input and output, but it’s enough to maintain the packets
pertaining to the same network flow.</p>
<p>Maintaining a total order of all packets may not even be preferably,
since it would prevent the system prioritize some packets, over
others. It could also cause head-of-line blocking, where a single
time-consuming-to-process packet prevents other packets from being
forwarded.</p>
<p>A requirement to maintain the packet order from <a class="reference internal" href="../glossary.html#term-Network-function"><span class="xref std std-term">network
function</span></a> input to output does not itself force the processing of
packets to happen in order.</p>
<p>In some cases, the actual work should (or must) be performed in the
same order the packets arrived. One example is PDCP sequence number
allocation, which generally must be performed in packet arrival order.</p>
<p>For a software work scheduler, it may be to more cycle-efficient to
maintain order by also imposing in-order processing (i.e., one flow
goes to one core, rather than one flow goes to many flows, and the
resulting packets are reordered to restore the original arrival
order).</p>
<p>Work ordering is one of the aspects that differentiates a data plane
work scheduler from a operating system process scheduler.</p>
<section id="scheduling-types">
<span id="id7"></span><h4>Scheduling Types<a class="headerlink" href="#scheduling-types" title="Permalink to this heading">¶</a></h4>
<p>This chapter reuses the terminology used by <a class="reference external" href="https://doc.dpdk.org/guides/prog_guide/eventdev.html">Eventdev</a>.</p>
<section id="atomic-scheduling">
<span id="id9"></span><h5>Atomic Scheduling<a class="headerlink" href="#atomic-scheduling" title="Permalink to this heading">¶</a></h5>
<p>Atomic scheduling means that work that a stream of items of work are
processed serially, in order, provided they belong to the same
<a class="reference internal" href="#flow"><span class="std std-ref">flow</span></a>.</p>
<p>Thus, two items of work belonging to the same flow will never be
processed in <a class="reference internal" href="../glossary.html#term-Parallelism"><span class="xref std std-term">parallel</span></a> on two different cores.</p>
<p>Atomic scheduling also does not imply that all items of work of some
particular flow will be scheduled to the same core, across time. For
example, the work scheduler is free to schedule item of work 0 of flow
0 to core 0, and item of work 1 of flow 0 on core 1, provided the
processing of item of work 0 has finished before the processing of
item of work 1 is initiated.</p>
<p>To avoid costly cache misses to flow-related data, it is often
beneficial to attempt to keep the same flow on the same core, provided
that core is not too busy.</p>
<p>Atomic scheduling may be used to guarantee ordering of items of work
(and often, in the end, packets from input to output).</p>
<p>Atomic scheduling allows the data plane application to safely access
data structure related the item of work’s flow (and that flow only),
without the use any further synchronization (e.g., a per-flow
<a class="reference internal" href="../glossary.html#term-Spinlock"><span class="xref std std-term">spinlock</span></a>).</p>
<p>Atomic scheduling does not imply that the processing of some
network-level stream of related packets (e.g., an IPsec tunnel) need
necessarily be completely serial. If such processing is divided up
into multiple stages, each pipeline stage gives some opportunity for
parallelism.</p>
</section>
<section id="ordered-scheduling">
<h5>Ordered Scheduling<a class="headerlink" href="#ordered-scheduling" title="Permalink to this heading">¶</a></h5>
<p>Ordered scheduling means that items of work from the same flow may be
scheduled to different cores and be processed in <a class="reference internal" href="../glossary.html#term-Parallelism"><span class="xref std std-term">parallel</span></a>.</p>
<p>The resulting items of work of such parallel processing, in the form
of more items of work, is reordered (i.e., reshuffle to fit the
original order of the items of work from which they derived), before
they progress to the next stage.</p>
<p>Ordered scheduling allows for a high degree of parallelism (i.e., the
use of many CPU cores and hardware accelerators) even when the flows
are few.</p>
</section>
<section id="parallel">
<h5>Parallel<a class="headerlink" href="#parallel" title="Permalink to this heading">¶</a></h5>
<p>Parallel scheduling means that the scheduler is free to schedule any
item of work to any core, in any order.</p>
<p>In parallel scheduling, the goal is simply to keep the cores busy.</p>
</section>
</section>
</section>
</section>
<section id="work-scheduling-models">
<h2>Work Scheduling Models<a class="headerlink" href="#work-scheduling-models" title="Permalink to this heading">¶</a></h2>
<section id="run-to-completion">
<span id="id10"></span><h3>Run to Completion<a class="headerlink" href="#run-to-completion" title="Permalink to this heading">¶</a></h3>
<p>XXX: Rename sequential?</p>
<p>This section describes a simple work scheduling model, which in its
most basic form, work isn’t really scheduled at all.</p>
<p>The run-to-completion model entails:</p>
<ol class="arabic simple">
<li><p>The use of <a class="reference internal" href="../threading/threading.html#data-plane-threading"><span class="std std-ref">data plane threading</span></a>.</p></li>
<li><p>Every <a class="reference internal" href="../glossary.html#term-EAL-thread"><span class="xref std std-term">EAL thread</span></a> poll a set of sources of work.</p></li>
<li><p>Upon retrieving an item of work (e.g., by retrieving a packet from
a <a class="reference internal" href="../glossary.html#term-NIC"><span class="xref std std-term">NIC</span></a> RX queue) the EAL thread will continue to
work on this task, without interruption, until it is finished.</p></li>
</ol>
<p><em>Finished</em> here means that all application-internal state changes
related a particular input has been performed, and any output related
to those set of inputs that can been produced, have been produced.</p>
<p>Outputs which cannot be produced because some information is not yet
available, or where the output must be produced at some particular
time, is exempted.</p>
<p>Thus, in a system implementing strict run-to-completion by this
definition, a thread can not hand off work to another thread, provided
the work could be performed by the original thread.</p>
<p>The archetypal example of a run-to-completion DPDK data plane
application is with a number of <a class="reference internal" href="../glossary.html#term-EAL-thread"><span class="xref std std-term">EAL threads</span></a>, one
per CPU core. Each thread is assigned one RX and TX NIC queue. Upon
receiving a packet, a thread picks up the packet, performs all the
processing required (e.g., runs all the network layers), and ends
producing a packet being sent out on one of the thread’s NIC TX
queues, without any interruptions.</p>
<p>A <a class="reference internal" href="../threading/threading.html#standard-threading"><span class="std std-ref">standard threading</span></a> model data plane
application using <a class="reference internal" href="../glossary.html#term-Preemptable-thread"><span class="xref std std-term">preemptable threads</span></a> and
blocking system function calls to access the network stack may seem to
run-to-completion from strictly source code point of view. However,
since the threads may be interrupted, such a design fails to qualify
as run-to-completion according to this definition. The same is true
for software using coroutines or other green threading techniques.</p>
<p>The driving force for this nearly interruption-free operation is
performance, living without multitasking has software
architecture-level impact.</p>
<section id="parallels-to-other-domains">
<h4>Parallels to Other Domains<a class="headerlink" href="#parallels-to-other-domains" title="Permalink to this heading">¶</a></h4>
<p>In general, run-to-completion is used to describe a system, where a
thread is assigned a task and continues execution until the task is
finished, without any interruptions.</p>
<p>In the context of operating system process scheduling,
run-to-completion is a synonym to cooperative multitasking, where a
thread is never <a class="reference internal" href="../glossary.html#term-Non-preemptable-thread"><span class="xref std std-term">preempted</span></a> and runs
until it voluntarily gives up the CPU. As outline in the
<a class="reference internal" href="../threading/threading.html#threading"><span class="std std-ref">Threading</span></a> chapter, data plane threads are never supposed to be
interrupted for any length of time, regardless if run-to-completion or
some other work scheduling model is used, so this usage does not make
sense if <a class="reference internal" href="../threading/threading.html#data-plane-threading"><span class="std std-ref">data plane threading</span></a> is used.</p>
<p>Run-to-completion in the context of finite state machine machines
means that a state machine finishes the processing of a particular
event, before it initiates processing of the next. Such state machines
are not parallel, while the data plane processing almost always is.</p>
<p>DPDK Eventdev maps more closely to an actor model, with the difference
that eventdev events are not the <em>only</em> means of communication between
actors (e.g., using shared memory is also allowed).</p>
<p>In a single-threaded UNIX application also employs run-to-completion,
in the sense no blocking system calls are made, and the processing of
an event either finishes, or the thread stores whatever state is
required for further, future, processing, and explicitly yields the
thread to allow it to be reused to process some other event. It’s not
run-to-completion in the sense that the threads are not preempted.</p>
</section>
<section id="properties">
<h4>Properties<a class="headerlink" href="#properties" title="Permalink to this heading">¶</a></h4>
<p>Pros</p>
<ul class="simple">
<li><p>Straight-forward implementation</p></li>
<li><p>No expensive hand-offs between cores</p></li>
<li><p>Trivial maintain input packet order</p></li>
</ul>
<p>Cons</p>
<ul class="simple">
<li><p>Higher larger data and instruction cache pressure than some
alternatives.</p></li>
<li><p>Must be extended with some load balancing mechanism to allow the use
of multiple CPU cores.</p></li>
<li><p>Contention for flow-level and/or network layer-related data
structures may be higher than alternatives.</p></li>
</ul>
</section>
</section>
</section>
<section id="pipeline">
<span id="id11"></span><h2>Pipeline<a class="headerlink" href="#pipeline" title="Permalink to this heading">¶</a></h2>
<p>In the pipeline architecture, the processing an arriving packet (or
some other input data plane application stimuli) is broken down into
several steps - referred to as <em>stages</em>.</p>
<p>Typically, a single item of work will be created to finish the first
stage of processing for a particular packet, then upon finishing
processing this stage, another will be created. This process continues
until the last stage has been reached. For applications where all
packets (or other stimuli resulting in work), all items of work could
be created, but if so is done, they most likely must be performed in
order.</p>
<p>The processing of each stage, for each packet, is an <a class="reference internal" href="../glossary.html#term-Item-of-work"><span class="xref std std-term">item of
work</span></a>.</p>
<p>Item of work being issued which are being issued to the work scheduler
are normally treated different if they are a contiation of a
processing already started for a packet, or a new job. This
distiniction is there allow the system for being set up in such a way,
that, in face of overload, tend to finish whatever multi-item-of-work
task which started, over starting new jobs.</p>
<section id="load-balancing">
<h3>Load balancing<a class="headerlink" href="#load-balancing" title="Permalink to this heading">¶</a></h3>
<p>The simplest model is static load balancing. Consider a data plane
application with two pipelines stages.</p>
</section>
<section id="parallism">
<h3>Parallism<a class="headerlink" href="#parallism" title="Permalink to this heading">¶</a></h3>
<p>Compared with the <a class="reference internal" href="#run-to-completion"><span class="std std-ref">run-to-completion</span></a> model,
provides parallelism for processing of one <em>flow</em>.</p>
<section id="flow-parallism">
<h4>Flow Parallism<a class="headerlink" href="#flow-parallism" title="Permalink to this heading">¶</a></h4>
</section>
<section id="packet-parallism">
<h4>Packet Parallism<a class="headerlink" href="#packet-parallism" title="Permalink to this heading">¶</a></h4>
<p>Usually, the domain logic processing requirements are so, a the
different stages for a particular packet cannot be paralleised,
because there is a dependency between stage &lt;N&gt; and stage &lt;N+1&gt;
processing.</p>
<p>For example, in a data plane application that terminates an IPsec
tunnel and certain TCP flow carried inside this tunnel, the IPsec
processing (e.g., decapsulation and decryption) must have completed
before TCP processing can commence.</p>
</section>
</section>
<section id="optimization-points">
<h3>Optimization Points<a class="headerlink" href="#optimization-points" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Minimize packet header and buffer meta data core-to-core transition</p></li>
<li><p>Minimize instruction cache footprint for a particular core</p></li>
<li><p>Minimize cache working set related to per-flow, per-stage data</p></li>
<li><p>Minimize cache working set related to per-stage data</p></li>
</ul>
</section>
<section id="scheduled-pipeline">
<h3>Scheduled Pipeline<a class="headerlink" href="#scheduled-pipeline" title="Permalink to this heading">¶</a></h3>
</section>
<section id="static-ingress-load-balancing">
<span id="scheduling-type"></span><span id="id12"></span><h3>Static Ingress Load Balancing<a class="headerlink" href="#static-ingress-load-balancing" title="Permalink to this heading">¶</a></h3>
<p>The source of work in a data plane is primarily incoming packets.
Packets come in a <a class="reference internal" href="../glossary.html#term-NIC"><span class="xref std std-term">NIC</span></a> RX queue, or the equivalent in case I/O
is virtualized (e.g., a memif port).</p>
<p>In case only a single NIC, with a single port, and a single RX queue
is available, the run-to-completion application cannot put more than
one <a class="reference internal" href="../glossary.html#term-Fast-path-lcore"><span class="xref std std-term">fast path lcore</span></a> to work.</p>
<p>In case the system has more ports, more cores may be used. The act of
&lt;mapping ports (as the associated RX queue) is a form of static load
balancing. In case all packets come in on one port, all but one of the
system’s CPU cores will remain idle.</p>
<p>Static ingress load balancing is a technique which can be used to turn
the basic run-to-completion into a more useful tool.  Both static and
<a class="reference internal" href="#dynamic-ingress-load-balancing"><span class="std std-ref">dynamic</span></a> may also be employed
by any of the pipeline models.</p>
<section id="receive-side-scaling">
<h4>Receive Side Scaling<a class="headerlink" href="#receive-side-scaling" title="Permalink to this heading">¶</a></h4>
<p>An early implementation of hardware-based ingress load balancing is
<a class="reference internal" href="../glossary.html#term-RSS"><span class="xref std std-term">Receive Side Scaling (RSS)</span></a>, first implemented on early
NICs with multiple RX queues.</p>
<p>The use of RSS requires that multiple RX queues are configured on the
NIC. When RSS is used in an operating system kernel, a one-to-one
correspondens between queue and CPU core is usually maintained, and
the queue-to-core mapping is static. However, in a data plane
application, it may be of value to have more RX queues than cores, to
allow for a limited form of load balancing.</p>
<p>When RSS is employed, the NIC identifies the flow of traffic to which
the packet belongs by examining specific fields in the packet’s header
(e.g., the source and destination IP addresses and transport protocol
port numbers).</p>
<p>The RSS function hash (usually a Toeplitz hash) those fields, and uses
the hash value as an index to a RX queue.</p>
<p>RSS can serve in the role of an <a class="reference internal" href="../glossary.html#term-Atomic-scheduling"><span class="xref std std-term">atomic</span></a>
type scheduler, where the item of work flow is computed by taking a
hash (e.g., of the packet’s source and destination IP addresses).</p>
<p>RSS works best in scenarios with many smaller flows (in the sense of
the RSS classifier). In scenario very few large RSS-level flows, there
is a substaional risk of uneven load distribution, where. In scenarios
where there is only a single input flow, RSS is of no use.</p>
<section id="collisions">
<h5>Collisions<a class="headerlink" href="#collisions" title="Permalink to this heading">¶</a></h5>
</section>
</section>
<section id="receive-flow-scaling">
<h4>Receive Flow Scaling<a class="headerlink" href="#receive-flow-scaling" title="Permalink to this heading">¶</a></h4>
</section>
<section id="dpdk-generic-flow">
<h4>DPDK Generic Flow<a class="headerlink" href="#dpdk-generic-flow" title="Permalink to this heading">¶</a></h4>
</section>
<section id="id13">
<h4>Properties<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h4>
</section>
</section>
</section>
<section id="dynamic-ingress-load-balancing">
<span id="id14"></span><h2>Dynamic Ingress Load Balancing<a class="headerlink" href="#dynamic-ingress-load-balancing" title="Permalink to this heading">¶</a></h2>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="vector" role="note">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>Such a list of packets is often referred to as a <em>vector</em>, as in
<a class="reference internal" href="../glossary.html#term-Vector-packet-processing"><span class="xref std std-term">vector packet processing</span></a>.</p>
</aside>
<aside class="footnote brackets" id="vpp" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p>Such a feature may not only be used to reduce work scheduling
overhead.  If the buffering also includes some clever reordering of
packets (i.e., work), so that packets pertaining to the same flow
are kept together, this will improve temporal locality of memory
accesses, and also allows for <a class="reference internal" href="../glossary.html#term-Vector-packet-processing"><span class="xref std std-term">vector packet processing</span></a>.</p>
</aside>
<aside class="footnote brackets" id="flow-affinity" role="note">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<p>Such scheduler behavior is sometimes referred to as maintaining
affinity, but is different from <a class="reference internal" href="../glossary.html#term-Processor-affinity"><span class="xref std std-term">processor affinity</span></a> in the
sense it’s not a strict requirement, rather a performance heuristic.</p>
</aside>
</aside>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Data Plane Software Design</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../threading/threading.html">Threading</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Work Scheduling</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-concepts">Basic Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#item-of-work">Item of Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="#work-scheduler">Work Scheduler</a></li>
<li class="toctree-l3"><a class="reference internal" href="#work-size">Work Size</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flow">Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="#work-ordering">Work Ordering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#work-scheduling-models">Work Scheduling Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#run-to-completion">Run to Completion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pipeline">Pipeline</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#load-balancing">Load balancing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parallism">Parallism</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimization-points">Optimization Points</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduled-pipeline">Scheduled Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="#static-ingress-load-balancing">Static Ingress Load Balancing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dynamic-ingress-load-balancing">Dynamic Ingress Load Balancing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../eth.html">Ethernet Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mbuf.html">The Packet Buffer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../headers.html">Protocol Header Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mem.html">Memory Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sync.html">Synchronization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cache.html">Caches</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datastructures.html">Data Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stats/stats.html">Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../time.html">Timekeeping</a></li>
<li class="toctree-l1"><a class="reference internal" href="../timers.html">Timers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../crypto.html">Cryptography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modularization.html">Modularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../control.html">Control Plane</a></li>
<li class="toctree-l1"><a class="reference internal" href="../slowpath.html">Slow Path</a></li>
<li class="toctree-l1"><a class="reference internal" href="../antipatterns.html">Anti Patterns</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../threading/threading.html" title="previous chapter">Threading</a></li>
      <li>Next: <a href="../eth.html" title="next chapter">Ethernet Devices</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Ericsson AB.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.0.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/work/work.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>